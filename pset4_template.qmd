---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*EL\*\* \*\*AS\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*1\*\* Late coins left after submission: \*\*2\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
List of variables we need: PRVDR_CTGRY_SBTYP_CD, PRVDR_CTGRY_CD, PRVDR_NUM, PGM_TRMNTN_CD, FAC_NAME, CITY_NAME, ST_ADR, ZIP_CD, SSA_CNTY_CD, STATE_CD

2. 
    a.
    ```{python}
    import pandas as pd
    import altair as alt
    ```

    ```{python}
    # Set path and read data
    path = r"/Users/evylanai/MPP Autumn '24/Python II/pos2016.csv"
    df2016 = pd.read_csv(path)
    ```

    ```{python}
    # Subset short term hospital
    shortterm_2016 = df2016[(df2016["PRVDR_CTGRY_SBTYP_CD"] == 1.0) & (df2016["PRVDR_CTGRY_CD"] == 1)]

    # How many hopsitals in this data
    print(len(shortterm_2016)) # Returns 7245

    ```

    b. In the research brief from the Kaiser Family Foundation, they cite that there are nearly 5,000 short-term, acute0-care hospitals in the U.S., and this estimation is different that what I found, which returned 7,245 short-term hospitals. The article wsa published in July of 2016, and I use data from Q4 of 2016, so the data from the KFF brief could be out of date. Additionally, they may be imposing additional parameters that are not listed in the brief, which could bring their total number of facilities down.

3. 
```{python}
# Create a year column for 2016 observations
shortterm_2016["YEAR"] = 2016
```

```{python}
# Repeat above steps for 2017
path = r"/Users/evylanai/MPP Autumn '24/Python II/pos2017.csv"
df2017 = pd.read_csv(path)

# Subset short term hospital
shortterm_2017 = df2017[(df2017["PRVDR_CTGRY_SBTYP_CD"] == 1.0) & (df2017["PRVDR_CTGRY_CD"] == 1)]

# Create a year column for 2017 observations
shortterm_2017["YEAR"] = 2017
```

```{python}
# Repeat above steps for 2018
path = r"/Users/evylanai/MPP Autumn '24/Python II/pos2018.csv"
# Added this encoding that ChatGPT provided when I encountered an error code
df2018 = pd.read_csv(path, encoding="ISO-8859-1")

# Subset short term hospital
shortterm_2018 = df2018[(df2018["PRVDR_CTGRY_SBTYP_CD"] == 1.0) & (df2018["PRVDR_CTGRY_CD"] == 1)]

# Create a year column for 2018 observations
shortterm_2018["YEAR"] = 2018
```

```{python}
path = r"/Users/evylanai/MPP Autumn '24/Python II/pos2019.csv"
# Added this encoding that ChatGPT provided when I encountered an error code
df2019 = pd.read_csv(path, encoding="ISO-8859-1")

# Subset short term hospital
shortterm_2019 = df2019[(df2019["PRVDR_CTGRY_SBTYP_CD"] == 1.0) & (df2019["PRVDR_CTGRY_CD"] == 1)]

# Create a year column for 2019 observations
shortterm_2019["YEAR"] = 2019
```

```{python}
# Create list of date frames to append
dfs_to_append = [shortterm_2016, shortterm_2017, shortterm_2018, shortterm_2019]
# Append them
appended_df = pd.concat(dfs_to_append, axis=0, ignore_index=True)
```

```{python}
# Aggregate the data by year
yearly_counts = appended_df.groupby("YEAR").size().reset_index(name = "count")

# Plot the aggregated data
alt.Chart(yearly_counts).mark_bar().encode(
    x=alt.X("YEAR:O", title="Year", axis=alt.Axis(labelAngle=0)),
    y=alt.Y("count:Q", title="Number of Observations", scale=alt.Scale(domain=[7225, 7300], clamp=True))
).properties(
    title="Number of Q4 Observations per Year",
    width=600,
    height=200
)

```

4. 
    a. 
    ```{python}
    # Group unique observations by year
    # Modify this line to count unique observations per year
    unique_byyear = appended_df.groupby("YEAR")["PRVDR_NUM"].nunique().reset_index(name = "count")

    alt.Chart(unique_byyear).mark_bar().encode(
        x=alt.X("YEAR:O", title="Year", axis=alt.Axis(labelAngle=0)),
        y=alt.Y("count:Q", title="Number of Unique Observations", scale=alt.Scale(domain=[7225, 7300], clamp=True))
    ).properties(
        title="Number of Unique Observations per Year (in Q4)",
        width=600,
        height=200
    )

    ```

    b. The plots for total observations by year (in quarter four) and unique observations by year (grouped by hospital identifier number) are identical. This means that each hospital only has one observation associated with it in the dataset, which could imply that that hospital is only assessed/reviewed once per quarter (or potentially, once per year).

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
#Load df
path = r'C:\Users\msald\OneDrive\Escritorio\UChicago\3. Classes\2425\1. Autum\2. PythonII\ps4\appended_df.csv'

df = pd.read_csv(path)

#df2017_inactive = df[df['YEAR'] == 2017 & df['PGM_TRMNTN_CD'] != 0]
#df2018_inactive = df[df['YEAR'] == 2018]
```



```{python}

# First, let's split our df by year
df_2016 = df[df['YEAR'] == 2016]
df_2017 = df[df['YEAR'] == 2017]
df_2018 = df[df['YEAR'] == 2018]
df_2019 = df[df['YEAR'] == 2019]

# Now, let's identify what hospitals were active in 2016
df_2016_active = df_2016[df_2016['PGM_TRMNTN_CD'] == 0]
df_2016_inactive = df_2016[df_2016['PGM_TRMNTN_CD'] != 0]


# Finally, let's compare across all the years
# For 2017
merged_2017 = pd.merge(df_2016_active, df_2017[['PRVDR_NUM',
                                                'PGM_TRMNTN_CD']],
                       on='PRVDR_NUM',
                       how='left',
                       suffixes=('_2016', '_2017'))


merged_2017['YEAR_CLOSURE'] = merged_2017.apply(
    lambda row: 2017 if row['PGM_TRMNTN_CD_2017'] != 0 else None, axis=1)

merged_2017_inactive = merged_2017[merged_2017['YEAR_CLOSURE'] == 2017].drop(
    ['YEAR',
     'PGM_TRMNTN_CD_2016',
     'PGM_TRMNTN_CD_2017'],
    axis=1
)

merged_2017_active = merged_2017[merged_2017['PGM_TRMNTN_CD_2017'] == 0]
len(merged_2017_inactive)

```

```{python}
# For 2018
merged_2018 = pd.merge(merged_2017_active, df_2018[['PRVDR_NUM',
                                                    'PGM_TRMNTN_CD']],
                       on='PRVDR_NUM',
                       how='left')


merged_2018 = merged_2018.rename(
    columns={'PGM_TRMNTN_CD': 'PGM_TRMNTN_CD_2018'})

merged_2018['YEAR_CLOSURE'] = merged_2018.apply(
    lambda row: 2018 if row['PGM_TRMNTN_CD_2018'] != 0 else row['YEAR_CLOSURE'], axis=1)

merged_2018_inactive = merged_2018[merged_2018['YEAR_CLOSURE'] == 2018].drop(
    ['YEAR',
     'PGM_TRMNTN_CD_2016',
     'PGM_TRMNTN_CD_2017',
     'PGM_TRMNTN_CD_2018'],
    axis=1
)

merged_2018_active = merged_2018[merged_2018['PGM_TRMNTN_CD_2018'] == 0]

len(merged_2018_inactive)
```

```{python}
# For 2019
merged_2019 = pd.merge(merged_2018_active, df_2019[['PRVDR_NUM',
                                                    'PGM_TRMNTN_CD']],
                       on='PRVDR_NUM',
                       how='left')


merged_2019 = merged_2019.rename(
    columns={'PGM_TRMNTN_CD': 'PGM_TRMNTN_CD_2019'})

merged_2019['YEAR_CLOSURE'] = merged_2019.apply(
    lambda row: 2019 if row['PGM_TRMNTN_CD_2019'] != 0 else row['YEAR_CLOSURE'], axis=1)

merged_2019_inactive = merged_2019[merged_2019['YEAR_CLOSURE'] == 2019].drop(
    ['YEAR',
     'PGM_TRMNTN_CD_2016',
     'PGM_TRMNTN_CD_2017',
     'PGM_TRMNTN_CD_2018',
     'PGM_TRMNTN_CD_2019'],
    axis=1
)

merged_2019_active = merged_2019[merged_2019['PGM_TRMNTN_CD_2019'] == 0]

suspected_closures = pd.concat([merged_2017_inactive,
                                merged_2018_inactive,
                                merged_2019_inactive],
                               ignore_index=True)

print(f"Number of suspected hospital closures by 2019: {
      len(suspected_closures)}")

```

2. 

```{python}
# For this step, we sorted suspected closures by facility name and report first 10 rows
suspected_closures_sorted = suspected_closures[['FAC_NAME', 'ST_ADR', 'ZIP_CD', 'YEAR_CLOSURE']].sort_values(by='FAC_NAME')
print(suspected_closures_sorted.head(10))

```

3. 

```{python}
active_2017_by_zcd = merged_2017_active.groupby('ZIP_CD').size().reset_index(name='active_2017')
active_2018_by_zcd = merged_2018_active.groupby('ZIP_CD').size().reset_index(name='active_2018')
active_2019_by_zcd = merged_2019_active.groupby('ZIP_CD').size().reset_index(name='active_2019')

zcd_comparison_1 = pd.merge(active_2017_by_zcd, active_2018_by_zcd, on='ZIP_CD', how='left').fillna(0)
zcd_comparison_1['comparison'] = zcd_comparison_1['active_2018'] - zcd_comparison_1['active_2017']
zcd_comparison_1 = zcd_comparison_1[zcd_comparison_1['comparison'] >= 0]

merged_2017_inactive_2 = pd.merge(merged_2017_inactive, zcd_comparison_1[['ZIP_CD',
                                                'comparison']],
                       on='ZIP_CD',
                       how='left')

merged_2017_inactive_clean = merged_2017_inactive_2[merged_2017_inactive_2['comparison'] != 0].drop(
    ['comparison'],
    axis=1
) 

zcd_comparison_2 = pd.merge(active_2018_by_zcd, active_2019_by_zcd, on='ZIP_CD', how='left').fillna(0)
zcd_comparison_2['comparison'] = zcd_comparison_2['active_2019'] - zcd_comparison_2['active_2018']
zcd_comparison_2 = zcd_comparison_2[zcd_comparison_2['comparison'] >= 0]

merged_2018_inactive_2 = pd.merge(merged_2018_inactive, zcd_comparison_2[['ZIP_CD',
                                                'comparison']],
                       on='ZIP_CD',
                       how='left')

merged_2018_inactive_clean = merged_2018_inactive_2[merged_2018_inactive_2['comparison'] != 0].drop(
    ['comparison'],
    axis=1
) 

```

    a.
How many hospitals fit the definition?

```{python}
merged_2017_inactive_drops = merged_2017_inactive_2[merged_2017_inactive_2['comparison'] == 0].drop(
    ['comparison'],
    axis=1
)

merged_2018_inactive_drops = merged_2018_inactive_2[merged_2018_inactive_2['comparison'] == 0].drop(
    ['comparison'],
    axis=1
)

merge_acq = pd.concat([merged_2017_inactive_drops,
                       merged_2018_inactive_drops],
                      ignore_index=True)
print(f"Number of merge/acq: {
      len(merge_acq)}")

```

After correcting for this, how many inactive hospitals do I have left?
```{python}
suspected_closures_clean = pd.concat([merged_2017_inactive_clean,
                                merged_2018_inactive_clean,
                                merged_2019_inactive],
                               ignore_index=True)



print(f"Number of real hospital closure by 2019: {
      len(suspected_closures_clean)}")


```

```{python}
#Para amor, borrar luego
path = r'C:\Users\msald\OneDrive\Escritorio\UChicago\3. Classes\2425\1. Autum\2. PythonII\ps4\suspected_closures_clean.csv'

suspected_closures_clean.to_csv(path, index=False)

```

    b.

```{python}
suspected_closures_clean_sorted = suspected_closures_clean[['FAC_NAME', 'ST_ADR', 'ZIP_CD', 'YEAR_CLOSURE']].sort_values(by='FAC_NAME')
print(suspected_closures_clean_sorted.head(10))
```

## Download Census zip code shapefile (10 pt) 

```{python}
import geopandas as gpd

```

```{python}

path = r'C:\Users\msald\OneDrive\Escritorio\UChicago\3. Classes\2425\1. Autum\2. PythonII\ps4\gz_2010_us_860_00_500k\gz_2010_us_860_00_500k.shp'

gdf = gpd.read_file(path)
```

1. 
    a.
    b. 
2. 

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
