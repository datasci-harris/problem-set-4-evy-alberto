---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*EL\*\* \*\*AS\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*1\*\* Late coins left after submission: \*\*2\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
List of variables we need: PRVDR_CTGRY_SBTYP_CD, PRVDR_CTGRY_CD, PRVDR_NUM, PGM_TRMNTN_CD, FAC_NAME, CITY_NAME, ST_ADR, ZIP_CD, SSA_CNTY_CD, STATE_CD

2. 
    a.
    ```{python}
    import pandas as pd
    import altair as alt
    ```

    ```{python}
    # Set path and read data
    path = r"/Users/evylanai/MPP Autumn '24/Python II/pos2016.csv"
    df2016 = pd.read_csv(path)
    ```

    ```{python}
    # Subset short term hospital
    shortterm_2016 = df2016[(df2016["PRVDR_CTGRY_SBTYP_CD"] == 1.0) & (df2016["PRVDR_CTGRY_CD"] == 1)]

    # How many hopsitals in this data
    print(len(shortterm_2016)) # Returns 7245

    ```

    b. In the research brief from the Kaiser Family Foundation, they cite that there are nearly 5,000 short-term, acute0-care hospitals in the U.S., and this estimation is different that what I found, which returned 7,245 short-term hospitals. The article wsa published in July of 2016, and I use data from Q4 of 2016, so the data from the KFF brief could be out of date. Additionally, they may be imposing additional parameters that are not listed in the brief, which could bring their total number of facilities down.

3. 
```{python}
# Create a year column for 2016 observations
shortterm_2016["YEAR"] = 2016
```

```{python}
# Repeat above steps for 2017
path = r"/Users/evylanai/MPP Autumn '24/Python II/pos2017.csv"
df2017 = pd.read_csv(path)

# Subset short term hospital
shortterm_2017 = df2017[(df2017["PRVDR_CTGRY_SBTYP_CD"] == 1.0) & (df2017["PRVDR_CTGRY_CD"] == 1)]

# Create a year column for 2017 observations
shortterm_2017["YEAR"] = 2017
```

```{python}
# Repeat above steps for 2018
path = r"/Users/evylanai/MPP Autumn '24/Python II/pos2018.csv"
# Added this encoding that ChatGPT provided when I encountered an error code
df2018 = pd.read_csv(path, encoding="ISO-8859-1")

# Subset short term hospital
shortterm_2018 = df2018[(df2018["PRVDR_CTGRY_SBTYP_CD"] == 1.0) & (df2018["PRVDR_CTGRY_CD"] == 1)]

# Create a year column for 2018 observations
shortterm_2018["YEAR"] = 2018
```

```{python}
path = r"/Users/evylanai/MPP Autumn '24/Python II/pos2018.csv"
# Added this encoding that ChatGPT provided when I encountered an error code
df2019 = pd.read_csv(path, encoding="ISO-8859-1")

# Subset short term hospital
shortterm_2019 = df2019[(df2019["PRVDR_CTGRY_SBTYP_CD"] == 1.0) & (df2019["PRVDR_CTGRY_CD"] == 1)]

# Create a year column for 2019 observations
shortterm_2019["YEAR"] = 2019
```

```{python}
# Create list of date frames to append
dfs_to_append = [shortterm_2016, shortterm_2017, shortterm_2018, shortterm_2019]
# Append them
appended_df = pd.concat(dfs_to_append, axis=0, ignore_index=True)
```

```{python}
# Aggregate the data by year
yearly_counts = appended_df.groupby("YEAR").size().reset_index(name = "count")

# Plot the aggregated data
alt.Chart(yearly_counts).mark_bar().encode(
    x=alt.X("YEAR:O", title="Year", axis=alt.Axis(labelAngle=0)),
    y=alt.Y("count:Q", title="Number of Observations", scale=alt.Scale(domain=[7225, 7300], clamp=True))
).properties(
    title="Number of Q4 Observations per Year",
    width=600,
    height=200
)

```

4. 
    a. 
    ```{python}
    # Group unique observations by year
    # Modify this line to count unique observations per year
    unique_byyear = appended_df.groupby("YEAR")["PRVDR_NUM"].nunique().reset_index(name = "count")

    alt.Chart(unique_byyear).mark_bar().encode(
        x=alt.X("YEAR:O", title="Year", axis=alt.Axis(labelAngle=0)),
        y=alt.Y("count:Q", title="Number of Unique Observations", scale=alt.Scale(domain=[7225, 7300], clamp=True))
    ).properties(
        title="Number of Unique Observations per Year (in Q4)",
        width=600,
        height=200
    )

    ```

    b. The plots for total observations by year (in quarter four) and unique observations by year (grouped by hospital identifier number) are identical. This means that each hospital only has one observation associated with it in the dataset, which could imply that that hospital is only assessed/reviewed once per quarter (or potentially, once per year).

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
# First, we identify all the Hospitals that were active in 2016
df_2016_active = df2016[df2016['PGM_TRMNTN_CD'] == '00=ACTIVE PROVIDER']


# Then, we create a function that helps us find hospitals that were not active 
# or disappeared by 2019
def check_closure(provider_num):
    df2019 = df2019[df2019['PRVDR_NUM'] == provider_num]
    if df2019.empty or df2019['PGM_TRMNTN_CD'].iloc[0] != '00=ACTIVE PROVIDER':
        return True
    return False

# Finally, we apply the closure check and create a suspected closures DataFrame
suspected_closures = df2016[df2016['PRVDR_NUM'].apply(check_closure)]
suspected_closures['YEAR_CLOSED'] = 2019
print(f" Otro 2019: {len(suspected_closures)}")


```

2. 

```{python}
# For this step, we sorted suspected closures by facility name and report first 10 rows
suspected_closures = suspected_closures[['FAC_NAME', 'ZIP_CD', 'YEAR_CLOSED']].sort_values(by='FAC_NAME')
print(suspected_closures.head(10))

```

3. 
    a.

```{python}
# Now, let's check if the number of active hospitals in a ZIP code decreased after suspected closure year

def is_potential_merger(row):
    zip_code = row['ZIP_CD']
    year_closed = row['YEAR_CLOSED']
    
    # Filter data for the ZIP code for the current and next year
    current_year_hospitals = hospitals_df[(hospitals_df['ZIP_CD'] == zip_code) & (hospitals_df['YEAR'] == year_closed)]
    next_year_hospitals = hospitals_df[(hospitals_df['ZIP_CD'] == zip_code) & (hospitals_df['YEAR'] == year_closed + 1)]
    
    # Check if the number of active hospitals has decreased
    active_current_year = current_year_hospitals[current_year_hospitals['PGM_TRMNTN_CD'] == 'Active Provider']
    active_next_year = next_year_hospitals[next_year_hospitals['PGM_TRMNTN_CD'] == 'Active Provider']
    
    return len(active_next_year) >= len(active_current_year)

# Apply the filter to remove potential mergers from the list of closures
suspected_closures['POTENTIAL_MERGER'] = suspected_closures.apply(is_potential_merger, axis=1)
confirmed_closures = suspected_closures[~suspected_closures['POTENTIAL_MERGER']]
potential_mergers = suspected_closures[suspected_closures['POTENTIAL_MERGER']]

print(f"Number of suspected mergers: {len(potential_mergers)}")
print(f"Number of confirmed closures after removing potential mergers: {len(confirmed_closures)}")

# Step 3.ii: Sort the corrected closures list by name and report the first 10 rows
confirmed_closures_sorted = confirmed_closures[['FAC_NAME', 'ZIP_CD', 'YEAR_CLOSED']].sort_values(by='FAC_NAME')
print(confirmed_closures_sorted.head(10))
```

    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a.
    b. 
2. 

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
